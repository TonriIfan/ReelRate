# recommend/recommender/train_itemcf_mysql.py
import os
import pandas as pd
from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from sklearn.metrics.pairwise import cosine_similarity
from recommend.models import RecommendedMovie, User, Movie
from django.db import transaction

# âœ… è·å–å½“å‰é¡¹ç›®è·¯å¾„ï¼Œå®šä½ JDBC jar åŒ…
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
JAR_PATH = os.path.join(BASE_DIR, "lib", "mysql-connector-j-8.0.33", "mysql-connector-j-8.0.33.jar")

def generate_itemcf_for_user_mysql(user_id: int, top_n: int = 10, sample_fraction: float = 0.01):


    print(f"ğŸ¯ ä½¿ç”¨ PySpark ä» MySQL ä¸ºç”¨æˆ· {user_id} ç”Ÿæˆæ¨è")

    BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    JAR_PATH = os.path.join(BASE_DIR, "lib", "mysql-connector-j-8.0.33", "mysql-connector-j-8.0.33.jar")
    print(f"ğŸ“¦ åŠ è½½ JDBC é©±åŠ¨è·¯å¾„ï¼š{JAR_PATH}")


    # âœ… é…ç½® Spark å†…å­˜é™åˆ¶é˜² OOM
    spark = SparkSession.builder \
        .appName("ItemCF-Recommend") \
        .config("spark.jars", JAR_PATH) \
        .config("spark.driver.memory", "4g") \
        .config("spark.executor.memory", "4g") \
        .getOrCreate()

    # âœ… ä» MySQL ä¸­è¯»å–è¯„åˆ†è¡¨
    df = spark.read.format("jdbc") \
        .option("url", "jdbc:mysql://localhost:3306/reelrate_db") \
        .option("dbtable", "recommend_rating") \
        .option("user", "root") \
        .option("password", "123456") \
        .option("driver", "com.mysql.cj.jdbc.Driver") \
        .load() \
        .select("user_id", "movie_id", "score")

    # âœ… å¯é€‰æŠ½æ ·ï¼ˆä»…ç”¨äºå¼€å‘è°ƒè¯•é˜¶æ®µï¼‰
    if 0 < sample_fraction < 1:
        print(f"ğŸ“‰ æŠ½æ ·æ•°æ®ï¼šä»…ä¿ç•™ {sample_fraction*100:.1f}% ç”¨äºæ¨èè®­ç»ƒ")
        df = df.sample(withReplacement=False, fraction=sample_fraction, seed=42)

    # âœ… è½¬ä¸º Pandas è¿›è¡Œ ItemCF
    pdf = df.toPandas()

    if user_id not in pdf['user_id'].unique():
        print(f"âš  ç”¨æˆ· {user_id} æ— è¯„åˆ†ï¼Œè·³è¿‡æ¨è")
        spark.stop()
        return

    # âœ… æ„å»ºè¯„åˆ†çŸ©é˜µå¹¶è®¡ç®—ç›¸ä¼¼åº¦
    matrix = pdf.pivot_table(index="movie_id", columns="user_id", values="score").fillna(0)
    sim_matrix = pd.DataFrame(
        cosine_similarity(matrix),
        index=matrix.index,
        columns=matrix.index
    )

    user_rated = pdf[pdf["user_id"] == user_id].set_index("movie_id")["score"]
    seen_movies = set(user_rated.index)

    scores = {}
    for seen_id, rating in user_rated.items():
        similar_items = sim_matrix[seen_id].drop(index=seen_movies)
        for target_id, sim in similar_items.items():
            scores[target_id] = scores.get(target_id, 0) + rating * sim

    top_items = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:top_n]

    # âœ… å†™å…¥æ¨èç»“æœ
    user = User.objects.get(id=user_id)
    RecommendedMovie.objects.filter(user=user).delete()

    batch = []
    for mid, score in top_items:
        try:
            movie = Movie.objects.get(movie_id=mid)
            batch.append(RecommendedMovie(user=user, movie=movie, score=score))
        except Movie.DoesNotExist:
            continue

    with transaction.atomic():
        RecommendedMovie.objects.bulk_create(batch)

    print(f"âœ… ä¸ºç”¨æˆ· {user_id} å†™å…¥æ¨è {len(batch)} æ¡")
    spark.stop()
